{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the text across the applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore ipykernel warning\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, module='ipykernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications = pd.read_parquet('../../Data/split_4_cleaned.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OpportunityId', 'ApplicationId', 'ExternalBriefDescription',\n",
       "       'ExternalDescription', 'Title', 'pass_first_step', 'Step_Category',\n",
       "       'Applicant_Job_Titles', 'Applicant_Job_Responsibilities',\n",
       "       'Applicant_Education', 'Applicant_Reported_Skills'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applications.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names\n",
    "applications.columns = ['opportunity_id', 'application_id', 'opportunity_brief_description',\n",
    "            'opportunity_description', 'opportunity_title', 'application_pass_first_step', 'application_step_category',\n",
    "            'application_ob_titles', 'application_job_responsibilities',\n",
    "            'application_education', 'application_reported_skills']\n",
    "# Remove duplicates for job_descriptions\n",
    "original_descriptions = applications.opportunity_description.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are all lists of strings\n",
    "application_cols = [col for col in applications.columns if 'application_' in col]\n",
    "opportunity_cols = [col for col in applications.columns if 'opportunity_' in col]\n",
    "\n",
    "applications['application_concat'] = applications[application_cols].astype(str).agg('--'.join,axis=1)\n",
    "\n",
    "applications['application_full_tokenized'] = applications['application_concat'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_cols.append(\"application_concat\")\n",
    "application_cols.append(\"application_full_tokenized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When cells have NoneType values in their lists, we have pre-processing errors. This will be remedied by changing it to a string saying 'Nothing'\n",
    "- we do this because an empty list could cause errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If column is object, it will become string\n",
    "for column in applications.columns:\n",
    "    if applications[column].dtype == object:\n",
    "        applications[column] = applications[column].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22053 entries, 66159 to 88211\n",
      "Data columns (total 13 columns):\n",
      " #   Column                            Non-Null Count  Dtype \n",
      "---  ------                            --------------  ----- \n",
      " 0   opportunity_id                    22053 non-null  string\n",
      " 1   application_id                    22053 non-null  string\n",
      " 2   opportunity_brief_description     22053 non-null  string\n",
      " 3   opportunity_description           22053 non-null  string\n",
      " 4   opportunity_title                 22053 non-null  string\n",
      " 5   application_pass_first_step       22053 non-null  bool  \n",
      " 6   application_step_category         22053 non-null  int64 \n",
      " 7   application_ob_titles             22053 non-null  string\n",
      " 8   application_job_responsibilities  22053 non-null  string\n",
      " 9   application_education             22053 non-null  string\n",
      " 10  application_reported_skills       22053 non-null  string\n",
      " 11  application_concat                22053 non-null  string\n",
      " 12  application_full_tokenized        22053 non-null  string\n",
      "dtypes: bool(1), int64(1), string(11)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "applications.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now let's replace None values in the specified columns\n",
    "# for col in [\"applicant_background\", \"applicant_major\"]:\n",
    "#     applications[col] = applications[col].apply(lambda cell: ['Nothing Here' if item is None else item for item in cell])\n",
    "#     applications[col] = applications[col].apply(lambda cell: [item for i, item in enumerate(cell) if i == 0 or item != 'Nothing Here'])\n",
    "# applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danmarino/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/danmarino/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on job_title\n",
      "working on job_description\n",
      "working on applicant_background\n",
      "working on applicant_titles\n",
      "working on applicant_skills\n",
      "working on applicant_major\n",
      "working on applicant_licenses\n",
      "working on applicant_degrees\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    text = re.sub(html_pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Strip HTML tags and convert to lowercase\n",
    "    text = strip_html_tags(text).lower()\n",
    "    \n",
    "    # Remove unwanted strings using compiled regex pattern\n",
    "    # This changed\n",
    "    unwanted_strings = ['sign on bonus', 'sign bonus',\n",
    "                         'full time', 'ft', 'part time', 'pt',\n",
    "                        'day shift', 'night shift', 'second shift',\n",
    "                         'third shift', 'first shift', 'none', 'nbsp', 'amp']\n",
    "    unwanted_pattern = re.compile('|'.join(map(re.escape, unwanted_strings)))\n",
    "    text = re.sub(unwanted_pattern, '', text)\n",
    "\n",
    "    # Tokenize the text and remove stopwords while lemmatizing\n",
    "    # this changed\n",
    "    tokenizer = RegexpTokenizer(r'\\b\\w+\\b')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokenizer.tokenize(text) if token not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply pre-processing to the columns that need it\n",
    "preprocessing_columns = ['job_title', 'job_description', 'applicant_background',\n",
    "                         'applicant_titles', 'applicant_skills', 'applicant_major',\n",
    "                         'applicant_licenses', 'applicant_degrees']\n",
    "\n",
    "for column in preprocessing_columns:\n",
    "    print(f\"working on {column}\")\n",
    "    applications[column] = applications[column].astype('string').apply(preprocess_text)\n",
    "\n",
    "applications = applications.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: applicant_background\n",
      "Unique data types: {<class 'str'>}\n",
      "Column: applicant_titles\n",
      "Unique data types: {<class 'str'>}\n",
      "Column: applicant_skills\n",
      "Unique data types: {<class 'str'>}\n",
      "Column: applicant_major\n",
      "Unique data types: {<class 'str'>}\n",
      "Column: applicant_licenses\n",
      "Unique data types: {<class 'str'>}\n",
      "Column: applicant_degrees\n",
      "Unique data types: {<class 'str'>}\n"
     ]
    }
   ],
   "source": [
    "# Re-run the test code to see if the data types have changed\n",
    "\n",
    "for col in applicant_cols:\n",
    "    print(\"Column:\", col)\n",
    "    dtypes_of_items = set()  # Use a set to store unique data types\n",
    "    for cell in applications[col]:\n",
    "        for item in cell:\n",
    "            if item is not None:\n",
    "                dtypes_of_items.add(type(item))\n",
    "            else:\n",
    "                dtypes_of_items.add(type(None))\n",
    "    print(\"Unique data types:\", dtypes_of_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want zero NoneType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications.to_pickle('../../Data/preprocessed_applications.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
