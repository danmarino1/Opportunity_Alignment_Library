{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore ipykernel warning\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, module='ipykernel')\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications = pd.read_pickle('../../Data/preprocessed_applications.pkl')\n",
    "\n",
    "def chunk_text(tokens, chunk_size):\n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        yield tokens[i:i + chunk_size]\n",
    "\n",
    "columns_to_chunk = ['job_description']\n",
    "\n",
    "# Apply chunking to columns\n",
    "for column in columns_to_chunk:\n",
    "    applications[f'{column}_chunked'] = applications[column].apply(lambda x: list(chunk_text(nltk.word_tokenize(x), 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '000', 'qualified', 'experienced', 'rn', 'position', 'working', 'evaluates', 'assigned', 'patient', 'plan', 'implement', 'document', 'nursing', 'care', 'assist', 'physician', 'examination', 'procedure', 'performs', 'various', 'patient', 'test', 'administers', 'medication', 'within', 'scope', 'practice', 'registered', 'nurse']\n",
      "['promotes', 'patient', 'independence', 'establishing', 'patient', 'care', 'goal', 'teaching', 'patient', 'family', 'understand', 'condition', 'medication', 'self', 'care', 'skill', 'relies', 'experience', 'judgment', 'plan', 'accomplish', 'goal', 'performs', 'variety', 'task', 'wide', 'degree', 'creativity', 'latitude', 'expected']\n",
      "['northwest', 'medical', 'center', 'comprised', '300', 'bed', 'hospital', 'four', 'urgent', 'care', 'facility', 'freestanding', 'emergency', 'center', 'large', 'physician', 'group', 'offering', 'variety', 'setting', 'work', 'every', 'location', 'dedicated', 'providing', 'safe', 'quality', 'patient', 'care', 'commitment']\n",
      "['employee', 'strives', 'provide', 'culture', 'teamwork', 'respect', 'appreciation', 'staff', 'whether', 'care', 'patient', 'directly', 'work', 'support', 'role', 'employee', 'appreciation', 'celebration', 'throughout', 'year', 'opportunity', 'growth', 'satisfaction', 'part', 'hospital', 'leading', 'way', 'accessible', 'convenient', 'healthcare']\n",
      "['tucson', 'northwest', 'great', 'place', 'work', 'nmc', 'accredited', 'joint', 'commission', 'equal', 'opportunity', 'employer', 'race', 'gender', 'disability', 'veteran', 'status', 'vevraa', 'federal', 'contractor', 'ndash', 'priority', 'referral', 'protected', 'veteran', 'requested', 'cb', 'indnwrn']\n"
     ]
    }
   ],
   "source": [
    "for chunk in applications['job_description_chunked'][0]:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danmarino/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"In this optimized version, we calculate the full_app_embedding only once before the loop in the calculate_similarity function. \\nAdditionally, we use list comprehensions to generate the chunk_texts list, \\nand we leverage NumPy's vectorized operations to calculate all chunk embeddings at once.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sima feedback:* Avoid redundant calculations: \n",
    "The function get_bert_embeddings is repeatedly calculating the embeddings for the full application text.\n",
    " Instead, we can calculate it once outside the loop and reuse it for each chunk.\n",
    "* Avoid unnecessary data conversions: \n",
    "In the calculate_similarity function, the cosine_similarity is being calculated for single elements in a loop.\n",
    " We can instead calculate it once for all chunks at once, which will be more efficient.\n",
    "* Use list comprehensions: \n",
    "List comprehensions are generally more readable and Pythonic than explicitly creating lists using loops.\n",
    "* Use vectorized operations: \n",
    "Whenever possible, use vectorized operations with libraries like NumPy to speed up computations.\n",
    "\"\"\"\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs[0][0].mean(0).detach().numpy()\n",
    "\n",
    "# For calculating the most similar chunk of a job_description to the applicants' application\n",
    "def calculate_similarity_to_chunks(text_list, full_app_text):\n",
    "    full_app_embedding = get_bert_embeddings(full_app_text)\n",
    "    chunk_texts = [\" \".join(chunk) for chunk in text_list]\n",
    "    chunk_embeddings = np.array([get_bert_embeddings(chunk_text) for chunk_text in chunk_texts])\n",
    "    similarities = cosine_similarity([full_app_embedding], chunk_embeddings)[0]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"chunk_number\": range(len(text_list)),\n",
    "        \"chunk_content\": chunk_texts,\n",
    "        \"full_app_content\": [full_app_text] * len(text_list),\n",
    "        \"similarity_score\": similarities\n",
    "    })\n",
    "    return df.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "\"\"\"In this optimized version, we calculate the full_app_embedding only once before the loop in the calculate_similarity function. \n",
    "Additionally, we use list comprehensions to generate the chunk_texts list, \n",
    "and we leverage NumPy's vectorized operations to calculate all chunk embeddings at once.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_number</th>\n",
       "      <th>chunk_content</th>\n",
       "      <th>full_app_content</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10 000 qualified experienced rn position worki...</td>\n",
       "      <td>['Haven Home Health\\nJan 2019 – present\\nRespo...</td>\n",
       "      <td>0.775138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>northwest medical center comprised 300 bed hos...</td>\n",
       "      <td>['Haven Home Health\\nJan 2019 – present\\nRespo...</td>\n",
       "      <td>0.762203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>employee strives provide culture teamwork resp...</td>\n",
       "      <td>['Haven Home Health\\nJan 2019 – present\\nRespo...</td>\n",
       "      <td>0.746683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>promotes patient independence establishing pat...</td>\n",
       "      <td>['Haven Home Health\\nJan 2019 – present\\nRespo...</td>\n",
       "      <td>0.725969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tucson northwest great place work nmc accredit...</td>\n",
       "      <td>['Haven Home Health\\nJan 2019 – present\\nRespo...</td>\n",
       "      <td>0.716732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_number                                      chunk_content   \n",
       "0             0  10 000 qualified experienced rn position worki...  \\\n",
       "2             2  northwest medical center comprised 300 bed hos...   \n",
       "3             3  employee strives provide culture teamwork resp...   \n",
       "1             1  promotes patient independence establishing pat...   \n",
       "4             4  tucson northwest great place work nmc accredit...   \n",
       "\n",
       "                                    full_app_content  similarity_score  \n",
       "0  ['Haven Home Health\\nJan 2019 – present\\nRespo...          0.775138  \n",
       "2  ['Haven Home Health\\nJan 2019 – present\\nRespo...          0.762203  \n",
       "3  ['Haven Home Health\\nJan 2019 – present\\nRespo...          0.746683  \n",
       "1  ['Haven Home Health\\nJan 2019 – present\\nRespo...          0.725969  \n",
       "4  ['Haven Home Health\\nJan 2019 – present\\nRespo...          0.716732  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the function\n",
    "df_similarity = calculate_similarity_to_chunks(applications['job_description_chunked'][0], applications['full_app'][0])\n",
    "df_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (842056546.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Above we see that we've been able to find the chunk that is most similar to the full opportunity. This helps us understand which part is the most important to compare to our applicantsm\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Above we see that we've been able to find the chunk that is most similar to the full opportunity. This helps us understand which part is the most important to compare to our applicantsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications.loc[applications['job_id'] == 'dmlXNFI0MEOlW+qh07E4iQ==']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a feature in which the similarity between the application and the opportunity is calculated using BERT embeddings\n",
    "- The similarity is calculated using the cosine similarity between the embeddings of the application and the opportunity\n",
    "- The applicants are sorted and then scored by their semantic similarity to the opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_index(chunk_content):\n",
    "    try:\n",
    "        return int(chunk_content.split('[')[0].replace(\" \", ''))\n",
    "    except ValueError:\n",
    "        return None # Return None or an appropriate default value for unexpected values\n",
    "\n",
    "# Add a new column to store the original index\n",
    "applications['original_index'] = applications.index\n",
    "\n",
    "# List to store the individual DataFrames\n",
    "all_job_applicant_similarity_reduced = []\n",
    "\n",
    "counter = 0\n",
    "for job in applications['job_id'].unique():\n",
    "    jobs_applications = applications.loc[applications['job_id'] == job].reset_index(drop=True)\n",
    "    jobs_applications['full_app'] = jobs_applications['original_index'].astype(str) + jobs_applications['full_app']\n",
    "    \n",
    "    # Define list_of_full_apps\n",
    "    list_of_full_apps = jobs_applications['full_app'].to_list()\n",
    "\n",
    "    job_applicant_similarity = calculate_similarity_to_chunks(list_of_full_apps, jobs_applications['job_description'][0])\n",
    "    job_applicant_similarity['job_id'] = job\n",
    "    job_applicant_similarity['application_original_index'] = job_applicant_similarity['chunk_content'].apply(extract_index)\n",
    "    job_applicant_similarity['similarity_to_job_posting_rank'] = range(1, job_applicant_similarity.shape[0] + 1)\n",
    "\n",
    "    # Define job_applicant_similarity_reduced\n",
    "    job_applicant_similarity_reduced = job_applicant_similarity[['application_original_index', 'similarity_score', 'similarity_to_job_posting_rank']]\n",
    "    \n",
    "    # Append to the list\n",
    "    all_job_applicant_similarity_reduced.append(job_applicant_similarity_reduced)\n",
    "\n",
    "    counter += 1\n",
    "    print(f\"{counter} done!\")\n",
    "\n",
    "# Concatenate all the individual DataFrames into a single large DataFrame\n",
    "final_job_applicant_similarity_reduced = pd.concat(all_job_applicant_similarity_reduced, ignore_index=True)\n",
    "\n",
    "final_job_applicant_similarity_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the distribution of similarity scores\n",
    "final_job_applicant_similarity_reduced.application_original_index.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge using the 'original_index' column and 'application_original_index' column\n",
    "final_applications = applications.merge(final_job_applicant_similarity_reduced, how='left', left_on='original_index', right_on='application_original_index')\n",
    "\n",
    "# You can drop the 'application_original_index' column if it's no longer needed\n",
    "final_applications = final_applications.drop(columns=['application_original_index', 'original_index'])\n",
    "\n",
    "final_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications.to_pickle(\"opportunities_and_applications.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
