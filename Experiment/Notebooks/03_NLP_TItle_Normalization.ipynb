{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import Vocab\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Load the Spacy tokenizer\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load the data\n",
    "normalized_titles_df = pd.read_pickle('../../Data/app_opp_normalized.pkl')[['NormalizedTitle', 'Title']]\n",
    "sample_df = pd.read_parquet('../../Data/split_1.parquet')\n",
    "training_data = pd.concat([normalized_titles_df[['Title', 'NormalizedTitle']], sample_df[['Title']].dropna()], ignore_index=True)\n",
    "# Drop rows with missing values in 'Title' or 'NormalizedTitle'\n",
    "training_data.dropna(subset=['Title', 'NormalizedTitle'], inplace=True)\n",
    "#make all cells strings\n",
    "training_data['Title'] = training_data['Title'].astype(str)\n",
    "training_data['NormalizedTitle'] = training_data['NormalizedTitle'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, validation_data = train_test_split(training_data, test_size=0.1) # Adjust the test_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "# Vocabulary building function\n",
    "def build_vocab(texts):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenize(text))\n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies\n",
    "title_vocab = build_vocab(training_data['Title'])\n",
    "normalized_title_vocab = build_vocab(training_data['NormalizedTitle'])\n",
    "\n",
    "# Convert Counters to Vocab objects\n",
    "title_vocab = Vocab(title_vocab)\n",
    "normalized_title_vocab = Vocab(normalized_title_vocab)\n",
    "\n",
    "# Numerical encoding function\n",
    "def numericalize(text, vocab):\n",
    "    return [vocab[token] for token in tokenize(text)]\n",
    "\n",
    "# Apply numerical encoding to data\n",
    "training_data['Title'] = training_data['Title'].apply(lambda x: numericalize(x, title_vocab))\n",
    "training_data['NormalizedTitle'] = training_data['NormalizedTitle'].apply(lambda x: numericalize(x, normalized_title_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TitleDataset(Dataset):\n",
    "    def __init__(self, titles, normalized_titles):\n",
    "        self.titles = titles\n",
    "        self.normalized_titles = normalized_titles\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.titles[idx]), torch.tensor(self.normalized_titles[idx])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TitleDataset(training_data['Title'].tolist(), training_data['NormalizedTitle'].tolist())\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    titles, normalized_titles = zip(*batch)\n",
    "    titles = [title.clone().detach() for title in titles]\n",
    "    normalized_titles = [ntitle.clone().detach() for ntitle in normalized_titles]\n",
    "    titles_padded = pad_sequence(titles, batch_first=True, padding_value=0)\n",
    "    normalized_titles_padded = pad_sequence(normalized_titles, batch_first=True, padding_value=0)\n",
    "    return titles_padded, normalized_titles_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_vocab_size)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # Encode\n",
    "        x = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.encoder(x)\n",
    "        \n",
    "        # Decode\n",
    "        y = self.embedding(y)\n",
    "        outputs, _ = self.decoder(y, (hidden, cell))\n",
    "        predictions = self.fc_out(outputs)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Create model\n",
    "model = Seq2SeqModel(len(title_vocab), len(normalized_title_vocab), embedding_dim=256, hidden_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss function (ignoring the padding index)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0) # Assuming padding index is 0\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.5429\n",
      "Epoch [2/10], Loss: 1.0027\n",
      "Epoch [3/10], Loss: 0.2334\n",
      "Epoch [4/10], Loss: 0.0639\n",
      "Epoch [5/10], Loss: 0.0240\n",
      "Epoch [6/10], Loss: 0.0124\n",
      "Epoch [7/10], Loss: 0.0077\n",
      "Epoch [8/10], Loss: 0.0056\n",
      "Epoch [9/10], Loss: 0.0045\n",
      "Epoch [10/10], Loss: 0.0038\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (titles, normalized_titles) in enumerate(train_loader):\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(titles, normalized_titles)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        targets = normalized_titles.view(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_function(predictions, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation dataset and DataLoader\n",
    "validation_dataset = TitleDataset(validation_data['Title'].tolist(), validation_data['NormalizedTitle'].tolist())\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to collect predictions and targets\n",
    "predictions, targets = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through validation data\n",
    "with torch.no_grad():\n",
    "    for titles, normalized_titles in validation_loader:\n",
    "        pred = model(titles, normalized_titles[:,:-1]) # Exclude the last token as input to the decoder\n",
    "        pred_indices = torch.argmax(pred, dim=-1)\n",
    "        predictions.extend(pred_indices.tolist())\n",
    "        targets.extend(normalized_titles[:,1:].tolist()) # Exclude the first token as it's the start token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.vocab.vocab.Vocab"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(normalized_title_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Counter' object has no attribute 'get_itos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Get integer-to-string mapping for normalized titles\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m normalized_title_itos \u001b[39m=\u001b[39m normalized_title_vocab\u001b[39m.\u001b[39;49mget_itos()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/job_desc_project/lib/python3.9/site-packages/torchtext/vocab/vocab.py:158\u001b[0m, in \u001b[0;36mVocab.get_itos\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mexport\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_itos\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m    154\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39m        List mapping indices to tokens.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mget_itos()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Counter' object has no attribute 'get_itos'"
     ]
    }
   ],
   "source": [
    "# Get integer-to-string mapping for normalized titles\n",
    "normalized_title_itos = normalized_title_vocab.get_itos()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
